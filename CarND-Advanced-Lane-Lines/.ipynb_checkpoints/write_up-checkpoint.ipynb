{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./Testout/distortion.jpg \"Distortion Correction\"\n",
    "[image2]: ./Testout/undistort_demo.jpg \"Road Transformed\"\n",
    "[image3]: ./Testout/thres_mask.jpg \"Binary Example\"\n",
    "[image4]: ./Testout/fit.jpg \"Warp Example\"\n",
    "[image5]: ./Testout/final.jpg \"Fit Visual\"\n",
    "[image6]: ./Testout/final_pipeline.jpg \"Output\"\n",
    "[video1]: ./project_video.mp4 \"Project Video Result\"\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "Basically I just used the sample code from the coding practice on course website. The following result can be found in the \"P4.ipynb\" file\n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "### Pipeline (single images)\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "\n",
    "The first step of the pipeline is the distortion correction. Again, the following result can be found in the \"P4.ipynb\" file\n",
    "![alt text][image2]\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.\n",
    "\n",
    "I used a combination of color and gradient thresholds to generate a binary image (Detailed steps can be found in the \"P4.ipynb file and\"). \n",
    "\n",
    "For the color thresholding, I tested two different color space(HLS and LAB) and used the \"H\" and \"S\" channels from HLS and \"L\" and \"B\" channels from LAB.\n",
    "\n",
    "For the gradient thresholding, I combined the direction and magnitude threhold as well as the sobel x and y gradient threshold. \n",
    "\n",
    "The result after apply all above is shown below and it can be found in the \"P4.ipynb\" file in the section of \"Demostrate the pipeline\". I also did a simple masking to choose the region of interest.\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "In stead of using \"varying\" nodes for the transformation that is used in the demo, I simply defined 8 fixed points, but the final result is still good.\n",
    "\n",
    "```python\n",
    "src = np.float32([[560, 485],\n",
    "                  [770, 485],\n",
    "                  [1100, 700],\n",
    "                  [240, 700]])\n",
    "dst = np.float32([[300, 250],\n",
    "                  [1000, 250],\n",
    "                  [1000, 700],\n",
    "                  [300, 700]])\n",
    "```\n",
    "The left image below shows the result of persective tranformation from the \"masked and thresholded\" result.\n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "After obtaining the persective transformation result, I applied the sliding window search as is taught in the course vedios and fitted a 2nd-order polynomial based on the points found (the red and blue points in the right image above). The basic idea is using the histogram of binary map in the vertical direction. If the previous steps are accomplished properly, we will see two peaks in the histogram which will be the positions of the lanes. Then we can choose the points around the two positions and update the postions based on the centroids of the chosen points. We begin the search from the bottom of the image and repeat this process upwards. In this way we will find all the points we need to fit the lanes.\n",
    "\n",
    "To do this, I also defined a class call \"Line\", which contained the coefficients of the fitting result (in the member \"current_fit\"). This class will also be used to implement the moving average filter in the later step. I stored 8 latest fittings in the class. The best fitting will be the average of the 8 lastest results. Every time when a new fitting is generated, I will compare the current fitting with the best fitting. If the difference between them is acceptable, the current result will be push into the list and the oldest fitting will be pop out. The best fit will be returned to draw lanes in the later step.\n",
    "\n",
    "The following code is part of the class and the full definition can be found in the \"P4.ipynb\". \n",
    "```python\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False \n",
    "        \n",
    "        # x values of the last 10 fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        \n",
    "        #average x values of the fitted line over the last 10 iterations\n",
    "        self.bestx = None     \n",
    "        \n",
    "        #polynomial coefficients averaged over the last 10 iterations\n",
    "        self.best_fit = None  \n",
    "        \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])] \n",
    "    .\n",
    "    .\n",
    "    .\n",
    "```\n",
    "\n",
    "The following image shows the final result of the image processing pipeline.\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "The detail process to compute radius of curvature of the lane is clearly shown in the course website. We can use the lane points at the bottom of the image to calculate the result. That is, the y-axis we need is always the image height and we can simply apply the equation to the coefficients we found for the best fitting results to get the radius. It is important to note that a unit conversion is needed to convert pixels into meters.\n",
    "\n",
    "Similarly, to calculate the deviation from the center of the road, we just need to calculate the x-coordinates of lanes at the bottom of the image. Again in this case, the y-coordinate is always the image height and what we need to do is applying the fitting model to find x-coordinates of the two lanes. The difference between average of the x-coordinates and half of the image width will be car's deviation from the road center.\n",
    "\n",
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "The code for the above process can be found in the \"P4.ipynb\" file and the following image is a demo for the final result.\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "Here's a [link to my project video result](./Testout/project.mp4) and \n",
    "here's a [link to the challenge video result](./Testout/challenge.mp4)\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "The color and gradient thresholding function is not perfect and can be improved. If the image is affected by strong light like the \"harder_challenge.mp4\", the thresholding result will be very terrible. The moving average filter I wrote need a good initialization to ensure a good performance, making the pipeline unstable with respect to the various and complicated real world situations.\n",
    "\n",
    "For me, the most critical part of this project is the color and gradient thresholding. If the result of this part is good, all steps after this will be very simple and robust. If I could have more time, I would try some more color space and thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
